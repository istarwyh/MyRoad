https://mp.weixin.qq.com/s/FtLxZPzPyt2Ccv9KHZHOGA Attention is all you need

GPT是首先将海量文本数据转换为统一的 token表示，然后送给Transformer模型进行预训练，最后根据下游任务进行“下一个token 的预测”。而Sora则是首先将海量视频数据转换为统一的patch表示，然后送给Diffusion Transformer模型进行预训练，最后根据下游任务进行“下一个Patch的预测”。

音频不像单词那样是离散的，」Shulman 解释说。「它是一种波，是一种连续的信号。」高品质音频的采样率通常是 44kHz 或 48kHz，这意味着「每秒处理 48,000 个 Token」，他补充道。「这是个巨大的挑战，对吧？因此，你需要想办法将其简化为更合理的处理方式。」但是，具体该怎么做呢？「这需要大量的工作，许多启发式方法，以及各种技巧和模型

Transformer 是建立在当前最适合仿生算法的并行矩阵乘法硬件上的仿生算法范式

针对「大模型只是通过像统计学一样的方式来模仿人类现有的知识和能力，没有办法超越人类」的质疑，Ilya Sutskever 认为，预测下一个 token 的质量，其实反映了其对于语言背后隐藏的语义和知识的理解程度。这不仅是统计，更是对世界本质的压缩和表达。token 预测大多基于 Transformer 架构，尤其是其仅解码器（Decoder-Only）变体。Transformer 通过自注意力（Self-Attention）机制，允许模型在生成每个新 token 时，考虑到之前所有 token 的上下文信息，从而生成更加准确和连贯的文本。在进行下一个 token 预测之前，文本首先需要被 token 化，即分解成模型可以理解的最小单位（即最小 token）。这些 token 随后被转换为嵌入向量，即在模型中的数值表示。为了让模型理解 token 的顺序，每个 token 的嵌入向量会与位置嵌入向量相加，这样模型就能够捕捉到序列中的位置信息。

Humanoid Locomotion as Next token Prediction 首先，研究者把仿人机器人的感觉运动轨迹视作类似于自然语言中的单词序列，将感觉输入（如来自传感器的数据）和运动输出（如电机指令）的输入轨迹 token 化，组成轨迹的「单词」和「句子」。

只要把语音的模态——就像之前做图片跟文本一样，把声音模型的 incoder（编码器），对齐到一个语言模型上去，就可以把语言和语音改造成一个成纯流式的交互，从而大大降低语音的交互延迟和提高真实性。

想让GPT把无穷无尽的猜词停下来其实很简单，只需要“扩展”一下语素表，让“停下来”这个操作是一个新的语素就行了。如此一来，GPT在猜词的时候，就会一直猜一直猜，猜到语素“停下来”才停下来。

人类的思考过程，通常是通过语言、图像等具体的媒介来完成的，思考在一定程度上是显性的，我们很难脱离语言来思考理解，甚至我们有时需要更深度的思考，会将逻辑二次精炼亦或是寻找更好的图示。但大模型却没有这种媒介，或者说这种显性的、反复推敲的“思想媒介”还并不明显，究其原因，目前的大模型推理过程更像是直觉推理，我们并没有教会大模型如何显性“思考”。

“king – man + woman = queen”，这个是 Tomas Mikolov 等人在 Word2Vec 研究中的一个很著名的结果。Word2Vec 中之所以可以做这样的“运算”是因为存在一个线性表示。线性表示假设的核心思想是：在向量空间中，不同的方向都承载着特定的含义，通过将不同方向的向量相加，我们就能表示出各种概念。这可能就是神经网络内部最基本的运作机制。这就是线性表示假设的核心。我们可以把它纯粹理解为向量空间的抽象概念，也可以把它理解为神经元激活模式的一种表现，但最本质的特点是：空间中的方向都具有实际的语义含义。**而且这个概念最关键的特性在于，这些概念可以叠加：我们可以独立地修改代表不同概念的不同的向量，比如性别、国别等，然后把这些向量叠加在一起，又形成一个新的完整的概念。**目前有大量证据表明，这种线性表征至少在神经网络中是极其普遍的现象。

JEPA（_ZP注：Joint Embedding Predictive Architecture，联合嵌入预测架构_）实际上是一种宏观架构，而不是Transformer的替代品。你可以在JEPA中使用Transformer。你可以在其中安排不同的模块，这些模块可以是Transformer，也可以是其他东西。**JEPA和Transformer并不是对立的概念，它们是正交的，可以共存。**

JEPA真正要替代的是当前主流的大型语言模型架构，这些架构在业界被称为自回归解码器架构或Transformer。OpenAI称它们为GPT（通用Transformer）。

GPT是一种特定的架构，它使用我之前描述的自监督学习技术进行训练：输入一个符号序列（比如文本或单词序列），然后训练系统。系统的组织方式是，为了预测输入中的某个单词，它只能查看该单词左侧的内容。这种架构被称为因果架构。如果你训练一个系统，输入一段文本并让它复现这段文本，那么实际上你是在隐式地训练它预测文本中的下一个单词。因此，你可以用这个系统自回归地生成一个接一个的单词，这就是大型语言模型的工作原理。AI也搞“位置歧视”（Where to show Demos in Your Prompt:A Positional Bias ofIn-Context Learning）：

1. **架构的“原罪”**：我们现在用的主流大模型，基本都是基于Transformer的因果解码器（causal-decoder LLMs） ，并且采用自回归的方式进行训练 。这意味着它在生成内容时，是一个词一个词往后蹦的，前面的内容会通过一种名为“自回归掩码（autoregressive masking）”的机制，极大地影响后面所有内容的生成 。更深度的机理解释工作发现，模型中存在一种被称为“归纳头（induction heads）”的特殊注意力机制 ，它会不成比例地将注意力集中在序列早期的Token上 。这使得模型天然就更关注它先看到的信息，就像人的“第一印象”一样，一旦形成就很难被后来信息完全覆盖。当小帅和女神的第一次约会表现得很下头或性缩力满满，今后即便再好大概也于事无补。

2. **训练数据的“惯性”**：用于训练这些模型的“指令微调（instruction-tuning）”数据集，本身在格式上可能就存在一些约定俗成的模式（比如，例子总是放在某个固定区域）。模型在学习海量数据的过程中，不知不觉地把这种“格式偏好”也当成了一条隐性规则给学了进去 ，从而在面对不同结构时，表现出我们观察到的位置偏见。

所以，下次当您感觉某个Prompt的效果总也上不去时，除了绞尽脑汁修改措辞、调整示例内容，不妨试试一个最简单、成本最低的操作**把您的示例（Demos）换个位置**。也许这个不经意的举动，就能让您的模型性能豁然开朗。这个研究提醒我们，Prompt工程的深度，远比我们想象的要深，它的物理结构本身，就是一个值得我们去精细打磨的关键参数。

现在，试着将这种方法应用到现实世界中。比如你想训练一个机器人来规划任务或预测世界上会发生什么，这种方法就不奏效了。如果你用视频帧代替单词，将这些帧转换为类似token的东西，然后尝试训练系统来预测视频中接下来会发生什么，这种方法效果很差。原因是世界上发生的很多事情根本无法预测。在高维空间（比如视频）中，表示“你无法准确预测接下来会发生什么”这一事实，本质上是一个数学上难以处理的问题。而在离散空间（比如文本）中，这是可能的——你无法准确预测下一个单词是什么，但可以预测所有可能单词的概率分布。

然而，对于视频，我们不知道如何表示所有可能视频帧的分布。因此，那些在文本、DNA序列和蛋白质上效果很好的技术，**在视频或其他自然信号上并不适用**。JEPA就是针对这个问题提出的解决方案。它的核心思想是：*不在输入空间中进行预测，而是训练系统学习输入的抽象表示，然后在该表示空间中进行预测。*事实证明，这是一种更好的问题表述方式。

例如，如果我用摄像机拍摄我们所在的房间，将摄像机对准一个位置，然后慢慢转动摄像机，最后停下来，问系统“接下来视频中会发生什么”。系统可能会预测摄像机会继续转动，但你无法预测摄像机旋转后视野中的所有细节。比如房间里有一株植物，墙上可能有一幅画，可能有人坐在那里。系统无法预测这些人会是什么样子，也无法预测植物的种类或地板的纹理。这些细节根本无法预测。如果你训练一个系统去预测这些细节，它会花费大量资源去尝试预测那些无法预测的东西，最终失败。
