https://mp.weixin.qq.com/s/FtLxZPzPyt2Ccv9KHZHOGA
Attention is all you need

GPT是首先将海量文本数据转换为统一的 token表示，然后送给Transformer模型进行预训练，最后根据下游任务进行“下一个token 的预测”。而Sora则是首先将海量视频数据转换为统一的patch表示，然后送给Diffusion Transformer模型进行预训练，最后根据下游任务进行“下一个Patch的预测”。

音频不像单词那样是离散的，」Shulman 解释说。「它是一种波，是一种连续的信号。」高品质音频的采样率通常是 44kHz 或 48kHz，这意味着「每秒处理 48,000 个 Token」，他补充道。「这是个巨大的挑战，对吧？因此，你需要想办法将其简化为更合理的处理方式。」但是，具体该怎么做呢？「这需要大量的工作，许多启发式方法，以及各种技巧和模型

Transformer 是建立在当前最适合仿生算法的并行矩阵乘法硬件上的仿生算法范式

针对「大模型只是通过像统计学一样的方式来模仿人类现有的知识和能力，没有办法超越人类」的质疑，Ilya Sutskever 认为，预测下一个 token 的质量，其实反映了其对于语言背后隐藏的语义和知识的理解程度。这不仅是统计，更是对世界本质的压缩和表达。
token 预测大多基于 Transformer 架构，尤其是其仅解码器（Decoder-Only）变体。Transformer 通过自注意力（Self-Attention）机制，允许模型在生成每个新 token 时，考虑到之前所有 token 的上下文信息，从而生成更加准确和连贯的文本。
在进行下一个 token 预测之前，文本首先需要被 token 化，即分解成模型可以理解的最小单位（即最小 token）。这些 token 随后被转换为嵌入向量，即在模型中的数值表示。为了让模型理解 token 的顺序，每个 token 的嵌入向量会与位置嵌入向量相加，这样模型就能够捕捉到序列中的位置信息。

Humanoid Locomotion as Next token Prediction
首先，研究者把仿人机器人的感觉运动轨迹视作类似于自然语言中的单词序列，将感觉输入（如来自传感器的数据）和运动输出（如电机指令）的输入轨迹 token 化，组成轨迹的「单词」和「句子」。

只要把语音的模态——就像之前做图片跟文本一样，把声音模型的 incoder（编码器），对齐到一个语言模型上去，就可以把语言和语音改造成一个成纯流式的交互，从而大大降低语音的交互延迟和提高真实性。

想让GPT把无穷无尽的猜词停下来其实很简单，只需要“扩展”一下语素表，让“停下来”这个操作是一个新的语素就行了。如此一来，GPT在猜词的时候，就会一直猜一直猜，猜到语素“停下来”才停下来。

人类的思考过程，通常是通过语言、图像等具体的媒介来完成的，思考在一定程度上是显性的，我们很难脱离语言来思考理解，甚至我们有时需要更深度的思考，会将逻辑二次精炼亦或是寻找更好的图示。但大模型却没有这种媒介，或者说这种显性的、反复推敲的“思想媒介”还并不明显，究其原因，目前的大模型推理过程更像是直觉推理，我们并没有教会大模型如何显性“思考”。